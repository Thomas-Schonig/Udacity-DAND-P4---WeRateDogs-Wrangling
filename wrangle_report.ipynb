{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAND - Data Wrangling Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Report\n",
    "Tom Schonig, February 26th 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project required wrangling data about the WeRateDogs twitter account, covering the full data wrangling lifecycle:\n",
    "    - Gathering; obtaining our data from (3) sources, in (3) formats.\n",
    "    - Assessing; reviewing our data for quality and tidiness issues, and documenting our observations.\n",
    "    - Cleaning; performing tasks to remediate the identified quality and tidiness issues.\n",
    "\n",
    "Prior to starting the data wrangling process, the project documentation was carefully reviewed to understand both the project's objectives and any constraints that needed to be explicitly considered (in addition to use of 'best practices' and meeting rubric criteria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "The below requirements were documented in the \"Project Motivation\" and \"Project Details\" pages:\n",
    " - Use only original rating tweets that have images; do not use retweets and non-rating tweets\n",
    " - At least (8) quality issues and (2) tidyness issues must be documented and remediated\n",
    " - At least (3) insights and (1) visualization must be produced\n",
    " - Written reports must be prepared:\n",
    "     - 300 - 600 words describing the wrangling efforts (named 'wrangle_report'; submitted in PDF or HMTL)\n",
    "     - 250-word minimum communicating insights and analyses (named 'act_report'; submitted in PDF or HTML)\n",
    " - Store the clean DataFrame in a CSV named 'twitter_archive_master.csv', as well as other tables required for tidiness\n",
    " - The \"WeRateDogs\" twitter archive must be downloaded manually and read into the .ipynb\n",
    " - The tweet image predictions must be requested programmatically using the provided URL\n",
    " - API tokens or credentials must not be included in final submission\n",
    " - Wrangling must capture each rating's:\n",
    "     - Count of retweets\n",
    "     - Count of favorite/\"like\" interactions\n",
    "     - Tweet ID\n",
    "\n",
    "Udacity requires the use of the (3) data sources below:\n",
    " - An twitter archive of the \"WeRateDogs\" account; provided in CSV format\n",
    " - Additional data from the Twitter API; using Tweepy\n",
    " - Image prediction data from a Udacity neural net; hosted in TSV format\n",
    "\n",
    "The below were explicitly listed as non-requirements:\n",
    "   - Full sanitization of all data\n",
    "   - Rating ratios > 1 are valid and do not need to be cleaned\n",
    "   - Tweets do not need to be gathered beyond August 1st 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    " \n",
    " ### Data Gathering\n",
    "     This phase was straightforward, where we followed explicit instructions on how to access each data source.\n",
    "     \n",
    "         - 'twitter_achive.csv' was manually uploaded to the project workspace per Udacity instruction, and read \n",
    "         into a Pandas DataFrame named 'twitter_archive'\n",
    "         \n",
    "         - The Twitter API was called using the Tweepy library, using the 'twitter_archive' tweet_ids as a parameter. \n",
    "         The tweet JSON data was written to a text file named 'tweet_json', which was then read back and assembled \n",
    "         into a new DataFrame called 'twitter_api_data'. \n",
    "             - Because the API calls took a long time, using \"try\" and \"except\" blocks were essential, as encountering \n",
    "             a record break with our 'twitter_archive' IDs caused the cell to error. This was learned the hard way.\n",
    "             \n",
    "         - The Udacity image prediction data was programmatically opened, read, and written locally. It seemed like \n",
    "         a useful exercise to add some extra logic, to check locally and only fetch the file if no copy existed locally.\n",
    "         The local copy is then read into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Assessment\n",
    "     The data was manually and programmatically reviewed for quality and tidiness. The below questions were framed as \n",
    "     guidance for inquiry.\n",
    "\n",
    "   #### Quality\n",
    "        - Completeness : Are there missing records or values within and between tables?\n",
    "        - Accuracy : Is there wrong data that conforms to each column's validation rules?\n",
    "        - Consistency : Is the same information represented consistently across sources?\n",
    "        - Validity : Does any data break validation rules required by our schema? Do they defy real-world constraints?\n",
    "        \n",
    "   #### Tidiness\n",
    "        - Does each column represent a distinct variable?\n",
    "        - Does each observation occupy its own row?\n",
    "        - Does each type of observation have its own table?\n",
    "\n",
    "    To answer these questions, I began looking at the column structure in each dataframe, the record counts, and the \n",
    "    proportion of NaN values. Thankfully, there was already a common key between the sources and no overlapping columns. \n",
    "    Likewise, though each dataframe had different record counts, they weren't major and obvious differences (ie. orders\n",
    "    of magnitude differences). I also checked for duplicates, to see if each record truely represented unique observations.\n",
    "    \n",
    "    I recorded some general observations around completeness, tidiness, and consistency, and then began looking at \n",
    "    individual columns. The archive data column names were fairly descriptive. I leaned on the value_counts() method to \n",
    "    get a cursory understanding how complex each column was, ie. whether it naturally lent to categorization or \n",
    "    validation rules, or if it was tracking less tractable information. I tried to consider the real-world context \n",
    "    for each column, in order to highlight major deviations or to inspire validation rules that could be used to\n",
    "    programmatically check each column (eg. proper nouns in the 'name' column should have at least one uppercase character).\n",
    "    \n",
    "    Programmatic checks were used to guide manual reviews, which were ultimately necessary to elicit some corrections \n",
    "    in the data. I would later loop back into assessment after cleaning the structural issues with the data, to drill-deeper\n",
    "    into accuracy, consistency, and validity issues.\n",
    "\n",
    "##### Observations\n",
    "\n",
    "Here are the consolidated notes from the assessment phase.\n",
    "\n",
    "Quality -\n",
    "\n",
    "     - The record counts do not match between data sources\n",
    "\n",
    "    'twitter_archive'\n",
    "        - \"source\" column mistypified; should be category. Needs to be parsed to be intelligible\n",
    "        - \"doggo\", \"floofer\", \"pupper\", \"puppo\" columns are strings; some dogs occupy multiple 'stages'\n",
    "        - Numerators and denominators appear to contain both inconsistent and inaccurate data\n",
    "        - There appears to be (137) duplicated \"expanded_urls\" values\n",
    "        - the timestamp columns are strings\n",
    "        - contains retweets and replies, ie. non-rating tweets\n",
    "        - Some columns contain NaNs: 'in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id',\n",
    "            'retweeted_status_user_id','retweeted_status_timestamp','expanded_urls'\n",
    "                * The NaNs in the 'in_reply...' and 'retweet...' columns will be helpful for identifying non-ratings tweets\n",
    "                * The explicit requirement is for original rating tweets with images - so missing urls may be dropped as \n",
    "                   well\n",
    "        - There are (109) values in the 'name' column that are all lowercase, which appear to be dirty data (as proper \n",
    "        nouns, these should have at least one capital)\n",
    "        - tweet_id \t832645525019123713 did not contain an actual dog rating\n",
    "\n",
    "    \n",
    "    'twitter_api_data'\n",
    "        - columns imported as strings instead of integers\n",
    "    \n",
    "    'image_predictions'\n",
    "        - Column headings are not intelligible\n",
    "        - There appears to be duplicated \"jpg_url\" fields\n",
    "\n",
    "Tidiness -\n",
    "\n",
    "    - image_predictions - \"p1\" , \"p2\" headings iterate because they are expressing another value: iteration of \n",
    "    the prediction for each dog\n",
    "    - 'twitter_api_data' should be consolidated with the 'twitter_archive' data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Cleaning\n",
    "     Once the major quality and tidiness issues were identified and documented, the wrangling exercise moved toward cleaning.\n",
    "     This required defining exactly how the data should be represented in our desired dataset, followed by coding and \n",
    "     testing solutions to confirm that we have remediated the issues.\n",
    "     \n",
    "     The cleaning process started with addressing completeness and tidiness issues, as to facilitate further cleaning \n",
    "     efforts and prevent potential rework or making the same amendments across multiple sources. Because our project \n",
    "     requirements documented a complete dataset as one containing only original ratings tweets with images, the \n",
    "     most appropriate place to begin cleansing was the 'extra' records in the archive data. Some parameters to flag these \n",
    "     were identified in the assessment phase:\n",
    "         - Tweets with data populated indicating that it was a retweet or reply (the presence of non-NaN values)\n",
    "         - Tweets without URLs, which indicate that the tweet had no image (the presence of NaNs)\n",
    "         - Tweets with duplicated URLs, suggesting it may not be an original ratings tweet\n",
    "         \n",
    "     These records were programmatically selected, quantified, reviewed, and dropped. The dataframe was re-assesed to \n",
    "     confirm that all columns now represented NaNs in-keeping with the above expectations. \n",
    "     \n",
    "     Then the API data was joined to the archive for tidiness, since the dataframe contained data that extended the \n",
    "     same observation unit / record type of the archive. Once joined, the consolidated dataframe was again checked for NaNs,\n",
    "     as it was expected that there would not be full matching of records. New NaNs were discovered, relating records \n",
    "     that did not fully overlap between data sources. These were again assessed, and eventually dropped for completeness.\n",
    "     \n",
    "     Addressing the last identified tidiness issue, the image prediction dataframe was restructured using the melt() and\n",
    "     concatenate() methods. This was necessary because the table had redundant columns for each iteration of predictions, \n",
    "     which were not distinct variables requiring their own columns. Likewise, rows existed for each tweet, where the purpose\n",
    "     of the table appeared to be tracking predictions. After melting and reassembling the dataframe, checks were performed \n",
    "     to ensure that the record counts on the new table matched expectation (3x more than the original) and the table's head\n",
    "     was printed for visual examination. \n",
    "     \n",
    "     To set expectations during for subsequent analyses, the prediction data was joined to the master archive data and\n",
    "     checked for NaNs / record breaks. If this dataset did not overlap with our archive, it may not be relevant for use in\n",
    "     analysis. We quantified the breaks at under 6% the new archive data, which gave comfort that the records could be \n",
    "     used later (with some understood limitations). \n",
    "     \n",
    "     Cleaning then proceeded to accuracy, validity and consistency issues. Data types were converted, and some fields \n",
    "     were parsed for intelligibility in subsequent analysis. Where project documentation had suggested the 'doggo', etc, \n",
    "     fields could treated as a 'dog stage' column, I did not agree with this approach and saw an opportunity to pass these\n",
    "     columns into linear regression models by converting them to binary integers. Accordingly, these columns were looped \n",
    "     over to make the changes, printing the value counts before and after to confirm the changes.\n",
    "     \n",
    "     I then iterated back into assessing the data, using extreme \n",
    "     values in the ratings field to check for quality issues, aprioristically expecting two major forces at play:\n",
    "        1) Transcription issues from programmatically extracting the ratings\n",
    "        2) Legitimately outlandish scores, which would make for amusing manual review\n",
    "    \n",
    "    Both expectations were met by the assessment. Where errors were observed, they were noted for future clean-up. Full \n",
    "    notes from assessment of all the data sources are below. \n",
    "\n",
    "### Cleaning Summary\n",
    "Two tidiness issues have been addressed:\n",
    "    1) Separation of the 'twitter_archive' and 'twitter_api_data' has been remediated by merging the two dataframes\n",
    "    2) Embedded variability in the columns of 'image_predictions' has been remediated by melting the dataframe and \n",
    "    combining the melts, so that each prediction iteration is denoted by a column value instead of separate headings\n",
    "\n",
    "Additionally, ten quality issues have been addressed:\n",
    "    - Only original rating tweets remain, arrived at by:\n",
    "        1) Dropping retweets and replies\n",
    "        2) Dropping tweets without images\n",
    "        3) Dropping (1) tweet containing a duplicated image URL\n",
    "        4) Dropping (1) tweet containing a GoFundMe solicitation, and no rating\n",
    "    - Converting datatypes where required for utility and consistency\n",
    "        5) Parsing out the HTML from the 'source' column and converting it to a categorical datatype\n",
    "        6) Replacing the strings in the 'doggo', 'floofer', 'puppo', and 'pupper' columns with integers and \n",
    "            converting the datatype\n",
    "        7) Converting the 'timestamp' and 'retweet_timestamp' columns to datetime datatypes\n",
    "        8) 'twitter_api_data' columns read as strings have been converted to integers\n",
    "    - Column headings are intelligible\n",
    "        9) Image prediction columns have been renamed for self-descriptiveness\n",
    "    - Cleansed dirty values from 'rating_numerator' and 'rating_denominator' columns through the identification of\n",
    "        both extreme and mis-proportioned values, manually identifed errors in programmatic extraction\n",
    "        10) Applied corrected numerator and denominator values from each tweet using a dictionary of tweet_ids and values\n",
    " \n",
    "From the original assessment observations, the below remain unaddressed:\n",
    "\n",
    "    1) Completeness - The record counts do not match between data sources. However, we have sanitized (2) of the (3) sources\n",
    "    to match completely, and have quantified the number of breaks with the image prediction data [it is acceptable]. The \n",
    "    new 'twitter_archive_master' dataframe is internally coherent, and we've kept the 'image_prediction_cleaned' data\n",
    "    separate for tidiness, as it represents a different type of observation (public tweet vs. prediction event).\n",
    "\n",
    "    2) There known data quality isuses in the 'name' column. However, we will not address this, as perfect data quality here \n",
    "    is not required by the project, and the utility of sanitizing this is likely not worth the effort. Effort here does \n",
    "    not seem fruitful toward improving completeness or tidiness, and anonymization of the dogs does not materially impact \n",
    "    our analyses. In fact, while there is no explicit requirement for dogs, certain types of analyses may require \n",
    "    the obfuscation or anonymization of any personally identifiable information\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Udacity DAND classroom materials\n",
    " - Stack Overflow : https://stackoverflow.com/\n",
    " - Stack Abuse : https://stackabuse.com/\n",
    " - Pandas documentation : https://pandas.pydata.org/pandas-docs/stable/index.html\n",
    " - Tweepy documentation : http://docs.tweepy.org/en/v3.5.0/\n",
    " - JSON documentation : https://docs.python.org/3/library/json.html\n",
    " - cran-r.project.org, 'Tidy Data' : https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html\n",
    " - docs.python.org, request documentation : http://docs.python-requests.org/en/master/user/quickstart/\n",
    " - docs.python.org, os documentation : https://docs.python.org/3/library/os.path.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
